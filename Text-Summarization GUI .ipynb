{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many people associate AI with a sentient robot, such as The Terminator. For instance, imagine how cool it would be to develop a sophisticated murderer. Others' primary concept of artificial intelligence is something that looks like a person, but is really a robot, sorry to burst your bubble, but that's not AI at all, actually. In fact, it's more akin to mechatronics engineering than AI. In simple terms, artificial intelligence refers to intelligence that is beyond human capability. Our first gift from biology is intelligence, which can be found in people. Nonetheless, humans' understanding of how intelligence works remains extremely limited, as it is still seen as something that can only be discovered by the truly gifted (for reals stop watching too much of Greek gods centered movies).Therefore, what is intelligence? Intelligence is an entity's capacity to deduce patterns from data. The rate at which these patterns may be extracted is quantified by the intelligence quotient (IQ). It is everywhere; from the faces we see every day to the patterns we create, data is how we make sense of our world. Using a child as an example, the first significant thing a child learns is language; by listening to a variety of people speak, raw data is dumped into the child's memory; as the brain develops and matures, the brain begins processing some of these words and essentially discovers patterns in them; this is why the words are typically incoherent at first. Every human being on the planet possesses some degree of intelligence; this is how we acquired knowledge of language, math, history, philosophy, and astronomy, among other things. Nobody would be able to learn anything without intelligence.Artificial intelligence was created to replicate and, in fact, be a clone of natural intellect, with the exception of speed and diversity. While artificial intelligence is orders of magnitude faster than natural intelligence, NI outperforms AI in terms of diversity (the extent to which intelligence generalizes to various domains). For the nerds out there, you may be interested to know that there are two major categories of artificial intelligence: Artificial Narrow Intelligence and Artificial General Intelligence.We've taught computers to perform practically any task imaginable (including how to use the F word #SorryHumans). When you hear about Siri, Google Assistant, or Alexa, you are hearing ANI (Artificial Narrow Intelligence) in action. ANI systems are trained to perform a single task flawlessly. When a typical Tesla is disassembled, numerous ANI systems are discovered to be operating concurrently. There is an ANI that is responsible for the cruise control feature; it essentially maintains the car's speed; another ANI constantly monitors the onboard camera for pedestrians and also measures the car's proximity to other vehicles; it then informs the cruise control ANI when to slow down or accelerate; and yet another ANI checks and predicts when the car will require maintenance. (If you're enjoying this debate #ThumbsUp, you might want to brush up on AGI and the Turing Test.)To assist you better comprehend artificial intelligence's potential, I'll explore how it has permeated all major engineering disciplines and how you can build domain expertise.Aerospace EngineeringIn the aerospace and aviation industry, there are ANI-powered drones used for post-production quality control by detecting problems on the aircraft. They may also be used for road maintenance to discover cracks in pavement and other defects. Using AI, SpaceX determines when is the best time to launch and return for their falcons, as well as predicting when parts of the aircraft will need to be serviced, whereas airlines like Alaska Airlines and FlyDubai are deploying AIs capable of predictive maintenance, using computers to calculate service intervals for aircraft parts instead of relying on a standard, 100- or 200-hour lifespan. When the Perseverance landed on Mars in February 18, 2021, artificial intelligence was employed to land and descend the rover (I had to write that in full, it was on my birthday lol).  The spacecraft's landing was entirely automated, and this computer, which was trained how to land and how to handle any issues that arise, had full control of the procedure. In light of the fact that your aerospace engineering background is remarkable, a notable AI capability will further assist you in separating yourself from the pack. Mechanical EngineeringWe all know that you're eager to go to work with your hands covered in motor oil and engine fluids. You have to realize, though, that every major vehicle manufacturer is already making large strides to introduce automation to their production lines. On a normal assembly line, an Engineer's deep knowledge of differential equations won't result in another's perspiration. Nah, there's a 90% chance you'll see a line robot with a jaw-mounted camera. It pinpoints the location of damage and assists with the repair. I'd choose robots who can work continuously all year long in place of human employees, who would join unions and go on strike due to how I had dismissed an engineer after the company lost over five million dollars because the engineer screwed up by fixing the wrong nut. Dear Mechanical Engineering students who get excited over the deep roar of a turbocharged Camero: if you want opportunities to come after you, obtain software/AI expertise especially in automation and control and then sit back and watch your career blossom (reminds me of how the entire assassin network went after John Wick, nice movie).  Civil EngineeringImagine going into an interview with the CEO, who is overweight, and telling him you would build an AI system that will monitor and assess highway operations and that will forecast when maintenance is needed with 99% accuracy. You might have become a civil engineer. Some of those traffic cameras that you see in movies and the news are used to measure traffic, but many are put in place to monitor traffic and count the number of vehicles on the road, which is collected monthly by bridge and highway repair teams. Basically, they pass all the information to an AI that conducts all the predictions and analysis for them (i mean who has that time lol). Putting cameras on construction sites is also a huge arena for AI, which can tell you if people are in places they shouldn't be, which lets you notify your contracted security.If you have a passion for civil engineering and the water, you should know that with AI's help, systems will predict runoff and disastrous flooding events which will save lives and property. Artificial intelligence has previously been applied to understand when different kinds of natural disasters, like tsunamis, earthquakes, and hurricanes, will occur. Students in civil engineering may benefit from learning computer vision and applying it to their skillset.Agricultural EngineeringFor true gentlemen, everything on the farm can be automated; pest control can be simply handled by a camera that recognizes birds and other prey and sounds an alarm if any approach the crops. Smart harvesting is another significant area of AI, and it entails accurately forecasting when crops will be ready to harvest (for you ladies, this is similar to the EDD you received/will receive from your doctor #wink). Automated agricultural disease recognition is another critical application. Essentially, we have drones that can survey an entire farmland and transmit data to a control room if they detect any crop disease they have been trained to recognize. Perhaps one of the earliest significant applications of artificial intelligence in engineering occurred in the field of agriculture, owing to the fact that the possibilities are virtually unlimited.My colleagues in agricultural engineering may choose to consider Computer Vision, Automation, and Control as desirable skill sets.Materials EngineeringTo increase the speed and accuracy of materials simulations or on-the-fly data analysis of high-throughput experiments, research on the application of AI in materials science has concentrated on fields such as accelerated and accurate predictions of phase diagrams, crystal structures, materials properties, development of interatomic potentials, and energy functionals.Traditionally, the primary way to identify, describe, or create new materials has been through experiments. In addition, experimental accuracy is highly resource intensive because of the time, equipment, and other resources it takes. Materials science researchers are using computational approaches like the density functional theory (DFT), Monte Carlo simulations, and molecular dynamics to investigate phase and composition space more quickly and thoroughly than was possible in the past.Investigating materials design has become cheaper due to a combination of experiments and computer models. Large-scale investigations of certain materials have become easier because to huge increases in processing capacity as well as advances in code creation. An integral part of this process is to be able to choose experimental candidates that will work well. The high-throughput testing has enabled a data explosion, which, in turn, makes it viable to employ AI approaches in the domain of materials research.Given that there is no definitive way to conclude what has been a lengthy yet fascinating discussion about the life and times of AI as it relates to engineering students, allow me to borrow a quote from the venerable Norman Vincent Peale: \"Nobody ever mastered any skill except through intense persistent and intelligent practice.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "#BEFORE THE GUI BEGAN!!!!!!!!!!!!!!!!!\n",
    "#############################################################\n",
    "\n",
    "#############################################################\n",
    "#THERE EXISTED THIS CODE FOR TEXT SUMMARIZATION\n",
    "#############################################################\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "#Function to split text into sentences by fullstop(.)\n",
    "'''def read_article(text):\n",
    "    \n",
    "    article = text.split(\". \")\n",
    "    sentences =[]\n",
    "    \n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\",\" \").split(\" \"))\n",
    "    \n",
    "    return sentences'''\n",
    "\n",
    "# Read the text and tokenize into sentences\n",
    "def read_article(text):\n",
    "    \n",
    "    sentences =[]\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        sentence.replace(\"[^a-zA-Z0-9]\",\" \")\n",
    "\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "# Create vectors and calculate cosine similarity b/w two sentences\n",
    "def sentence_similarity(sent1,sent2,stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    \n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    #build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if not w in stopwords:\n",
    "            vector1[all_words.index(w)]+=1\n",
    "    \n",
    "    #build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if not w in stopwords:\n",
    "            vector2[all_words.index(w)]+=1\n",
    "            \n",
    "    return 1-cosine_distance(vector1,vector2)\n",
    "\n",
    "# Create similarity matrix among all sentences\n",
    "def build_similarity_matrix(sentences,stop_words):\n",
    "    #create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences),len(sentences)))\n",
    "    \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1!=idx2:\n",
    "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2],stop_words)\n",
    "                \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# Generate and return text summary\n",
    "def generate_summary(text,top_n):\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "    \n",
    "    # Step1: read text and tokenize\n",
    "    sentences = read_article(text)\n",
    "    \n",
    "    # Steo2: generate similarity matrix across sentences\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences,stop_words)\n",
    "    \n",
    "    # Step3: Rank sentences in similarirty matrix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    \n",
    "    #Step4: sort the rank and place top sentences\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n",
    "    \n",
    "    # Step 5: get the top n number of sentences based on rank    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(ranked_sentences[i][1])\n",
    "    \n",
    "    # Step 6 : output the summarized version\n",
    "    return \" \".join(summarize_text)\n",
    "\n",
    "############################################################################################\n",
    "#On this note we ended the long line of code responsible for Text Summarization\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#Before we move on, let us initialize codes for our keywords extraction too\n",
    "############################################################################################\n",
    "def extract(top_n, nr_candidates, doc):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    n_gram_range = (1, 1)\n",
    "    stop_words = \"english\"\n",
    "\n",
    "    # Extract candidate words/phrases\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "    candidates = count.get_feature_names()\n",
    "\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    doc_embedding = model.encode([doc])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    top_n = 10\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "        # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]\n",
    "\n",
    "###########################################################################################################################\n",
    "#Keywords Extraction Done\n",
    "###########################################################################################################################\n",
    "\n",
    "import tkinter as tk\n",
    "import center_tk_window\n",
    "import tkinter.font as TkFont\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "import docx\n",
    "\n",
    "win = tk.Tk()\n",
    "#win.configure(background='black')\n",
    "win.geometry(\"1300x650\")\n",
    "#win.configure(bg='gr)\n",
    "win.title('Text Summarization and Keyword Extraction')\n",
    "win.resizable(False, False)\n",
    "center_tk_window.center_on_screen(win)\n",
    "font = (\"Helvetica\", 10)\n",
    "font2 = (\"Segoe Boot Semilight\", 7)\n",
    "\n",
    "helv36 = TkFont.Font(family=\"Helvetica\",size=36,weight=\"bold\")\n",
    "\n",
    "\n",
    "Frame1 = tk.Frame(win, width=1300, height=30, bg='white')\n",
    "Frame1.grid_propagate(0)\n",
    "Frame1.grid(row=0)    \n",
    "\n",
    "Frame2 = tk.Frame(win, width=570, height=600, bg='white')\n",
    "Frame2.grid_propagate(0)\n",
    "Frame2.grid(row=1, sticky = tk.W)\n",
    "\n",
    "Frame3 = tk.Frame(win, width=160, height=600, bg='white')\n",
    "Frame3.grid_propagate(0)\n",
    "Frame3.grid(row=1)    \n",
    "\n",
    "Frame4 = tk.Frame(win, width=570, height=600, bg='white')\n",
    "Frame4.grid_propagate(0)\n",
    "Frame4.grid(row=1, sticky=tk.E)    \n",
    "\n",
    "Frame5 = tk.Frame(win, width=1300, height=20, bg='black')\n",
    "Frame5.grid_propagate(0)\n",
    "Frame5.grid(row=2, sticky=tk.S)    \n",
    "\n",
    "\n",
    "user_text = scrolledtext.ScrolledText(Frame2, width=68, height=37, wrap=tk.WORD, relief='solid')\n",
    "user_text.grid(column=0, row=0, columnspan=3, padx=(5,5))\n",
    "\n",
    "output = scrolledtext.ScrolledText(Frame4, width=68, height=37, wrap=tk.WORD, relief='solid')\n",
    "output.grid(column=0, row=0, columnspan=3, padx=(5,5))\n",
    "\n",
    "def openFile():\n",
    "    global filename\n",
    "    filename = askopenfilename()  \n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(filename) as f:\n",
    "            text = f.read()\n",
    "            user_text.insert(tk.INSERT, text)\n",
    "    elif filename.endswith(\".pdf\"):\n",
    "        import PyPDF2\n",
    "         # creating a pdf file object\n",
    "        pdfFileObj = open(filename, 'rb')\n",
    "        # creating a pdf reader object\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        # printing number of pages in pdf file\n",
    "        pages = pdfReader.numPages\n",
    "        num = 0\n",
    "        while num <= pages:\n",
    "            print(num)\n",
    "            pageObj = pdfReader.getPage(num)\n",
    "            user_text.insert(tk.INSERT, pageObj.extractText())\n",
    "            print(pageObj.extractText())\n",
    "            num+=1\n",
    "            if num == pages:\n",
    "                pdfFileObj.close()\n",
    "                break\n",
    "    elif filename.endswith((\"doc\", \"docx\")):\n",
    "        from docx import Document\n",
    "        doc = Document(filename)\n",
    "        for para in doc.paragraphs:\n",
    "            user_text.insert(tk.INSERT, para.text)\n",
    "def clear_text():\n",
    "    user_text.delete(\"1.0\", \"end\")\n",
    "    output.delete(\"1.0\", \"end\")\n",
    "    \n",
    "def summarize():\n",
    "    text=user_text.get(\"1.0\",\"end\")\n",
    "    print(text)\n",
    "    a = generate_summary(text, top_n=5)\n",
    "    output.insert(tk.INSERT, a)\n",
    "\n",
    "def extract_keywords():\n",
    "    text = user_text.get(\"1.0\", \"end\")\n",
    "    b = \"\\n *****************************KEYWORDS*************************** \\n\" + str(extract(top_n=10, nr_candidates=20, doc=text)) + \"\\n\"\n",
    "    output.insert(tk.INSERT, b)\n",
    "\n",
    "#Import\n",
    "image0 = Image.open(\"button_import.png\")\n",
    "image0 = image0.resize((140, 45))\n",
    "click_btn0 = ImageTk.PhotoImage(image0)\n",
    "img_label0= tk.Label(image=click_btn0)\n",
    "import_= tk.Button(Frame3, image=click_btn0, borderwidth=0, command=openFile)\n",
    "import_.grid(column=0, row=0, pady=(0,50), padx=(10,0))\n",
    "\n",
    "#Summarize Button\n",
    "image = Image.open(\"button_summarize.png\")\n",
    "image = image.resize((140, 45))\n",
    "click_btn = ImageTk.PhotoImage(image)\n",
    "img_label= tk.Label(image=click_btn)\n",
    "summarize= tk.Button(Frame3, image=click_btn, borderwidth=0, command=summarize)\n",
    "summarize.grid(column=0, row=0, pady=(80,0), padx=(10,0))\n",
    "#summarize = tk.Button(Frame3, height=10, width=40, text =\"Hello\", image=photo)\n",
    "\n",
    "#Extract Keywords\n",
    "image2 = Image.open(\"button_extract-keywords.png\")\n",
    "image2 = image2.resize((140, 45))\n",
    "click_btn2 = ImageTk.PhotoImage(image2)\n",
    "img_label2= tk.Label(image=click_btn2)\n",
    "extract_keywords= tk.Button(Frame3, image=click_btn2, borderwidth=0, command=extract_keywords)\n",
    "extract_keywords.grid(column=0, row=0, pady=(200,0), padx=(10,0))\n",
    "\n",
    "#Clear\n",
    "image3 = Image.open(\"button_clear.png\")\n",
    "image3 = image3.resize((140, 45))\n",
    "click_btn3 = ImageTk.PhotoImage(image3)\n",
    "img_label3= tk.Label(image=click_btn3)\n",
    "clear= tk.Button(Frame3, image=click_btn3, borderwidth=0, command=clear_text)\n",
    "clear.grid(column=0, row=0, pady=(317,0), padx=(10,0))\n",
    "\n",
    "\n",
    "\n",
    "# Making the text read only\n",
    "\n",
    "tk.Label(Frame5, text=\"Designed with love by: \", font=font2, bg='black', fg='white').grid(column=0, row=1, padx=(600,0), pady=(2,0))\n",
    "win.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
