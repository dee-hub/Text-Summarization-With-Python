{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"It is, perhaps, dreadfully apt that an invasion which began 20 years ago as a counter-terrorism operation has ended in the horror of a mass casualty terrorist attack. The US-led attempt to destroy al-Qaida and rescue Afghanistan from the Taliban was undercut by the Iraq war, which spawned Islamic State. Now the circle is complete as an Afghan IS offshoot emerges as America’s new nemesis.\n",
    "The Kabul airport atrocity shows just how difficult it is to break the cycle of violence, vengeance and victimisation. Joe Biden’s swift vow to hunt down the perpetrators and “make them pay” presumably means US combat forces will again be in action in Afghanistan soon. If the past is any guide, mistakes will be made, civilians will die, local communities will be antagonised. Result: more terrorists.\n",
    "\n",
    "It is an obvious irony that US military chiefs in Kabul are collaborating with the Taliban, their sworn enemy, against the common IS foe as the evacuation ends. This suggests negotiators, on both sides, could have tried harder to reach a workable peace deal. It may augur well for future cooperation, for example on humanitarian aid. But the Taliban has many faces – and many cannot be trusted.\n",
    "\n",
    "Last week’s events have raised yet more questions about Biden’s judgment and competence. He will be blamed personally. His predicament recalls the downfall of another Democratic president, Jimmy Carter. After the disastrous failure of Operation Eagle Claw to rescue US hostages in Tehran in April 1980, Carter was voted out of office the following November.\n",
    "\n",
    "Biden faces Republican calls to resign. His approval ratings have plunged. But he defiantly insists that quitting Afghanistan is the right thing to do. Polls suggest most Americans agree, though they are critical of how it has been managed. Unlike in Carter’s time, the next presidential election is three years away. By then the agony and humiliations of recent days may be a distant memory.\n",
    "\n",
    "The Kabul debacle also casts doubt on Biden’s new counter-terrorism strategy, which reportedly downgrades the threat posed by Islamist terrorism to the US. His national security team wants to shift global priorities and resources to meet different, 21st-century challenges to American hegemony, such as China, cyberwarfare and the climate crisis.\n",
    "\n",
    "Biden is said to want to use the 20th anniversary of the 11 September al-Qaida attacks on New York and Washington to declare America’s “forever wars” over – for which he will claim credit. Setting the Afghan shambles aside, he is expected to say the era of invasion, occupation, nation-building and the “global war on terror” is at an end. “The US approach should centre on gathering intelligence, training indigenous forces, and maintaining air power as well as special forces capabilities for the occasional strike when necessary,” foreign policy analysts Bruce Riedel and Michael O’Hanlon argued recently.\n",
    "\n",
    "No one knows whether such a costly, hard to organise strategy will work in the long term. But the shift is already having tangible consequences. In Iraq, for example, US combat operations will cease in December. About 2,500 Americans will stay, to train and advise. In Syria, a small number of special forces will remain. Iraqis understandably worry about an IS comeback and an Afghan-style implosion. The same story of US disengagement and drawback is heard across the Middle East as the US “pivots” to Asia. Combat aircraft are being redeployed, carrier battlegroups may be reassigned to the Pacific theatre, and anti-missile batteries are being withdrawn from Iraq, Kuwait, Jordan and Saudi Arabia. Most of these assets were pointed at Iran, deemed a prime sponsor of terrorism.\n",
    "\n",
    "In the Sahel, west Africa, the Democratic Republic of the Congo and Mozambique, the US barely registers in the fight against Boko Haram and assorted IS and al-Qaida affiliates. The impressively named US Africa Command is headquartered in Stuttgart. President Muhammadu Buhari warns that Nigeria could suffer a similar fate to Afghanistan without a “comprehensive partnership” with the US. “Some sense the west is losing its will for the fight,” he said. For US allies, all this points to a new era of enforced self-sufficiency and greater uncertainty. While Islamist-inspired attacks in the US have been rare since 9/11, in Europe many hundreds have died. Yet collective European counter-terrorism efforts often lack a military cutting edge. An exception was France’s ill-supported Operation Barkhane in Mali – until it was halted this year after suffering many casualties for little gain.\n",
    "\n",
    "The chaos in Afghanistan has vividly dramatised the ongoing threat from international terrorism. With up to 10,000 foreign Islamist fighters in the country, according to the UN, fears grow it will again become a launchpad for global jihad. So the prospect of a less directly engaged, homeland-focused American counter-terrorism approach is alarming for partners dependent on US leadership and protection.\n",
    "\n",
    "European Nato allies, sniping at Biden, are in denial. They don’t want to admit his Afghan withdrawal is just the start of something bigger. And as recent events painfully demonstrate, the UK is not remotely able to fend for itself.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53defd937c164403b8c449621b66f9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=265486777.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41619d79d5314869a67f77eec138ce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=53.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e0553aa4fc40a8afa85e09c18f03f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d57879f7f3b432eb84fc5b7f86d102a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadffe9ffc8c44a4a2a8c3dd5bfbcc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6137c7dfec174292beb2031b93013b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fb38830a254a5b82db302f588f7ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=190.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 10\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battlegroups',\n",
       " 'kabul',\n",
       " 'wars',\n",
       " 'vengeance',\n",
       " 'afghanistan',\n",
       " 'war',\n",
       " 'taliban',\n",
       " 'terrorism',\n",
       " 'terrorists',\n",
       " 'terrorist']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Sum Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum sum distance between pairs of data is defined as the pairs of data for which the distance between them is maximized. In our case, we want to maximize the candidate similarity to the document whilst minimizing the similarity between candidates.\n",
    "To do this, we select the top 20 keywords/keyphrases, and from those 20, select the 5 that are the least similar to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nemesis',\n",
       " 'iraqis',\n",
       " 'afghan',\n",
       " 'casualties',\n",
       " 'iraq',\n",
       " 'battlegroups',\n",
       " 'kabul',\n",
       " 'vengeance',\n",
       " 'afghanistan',\n",
       " 'terrorist']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=10, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final method for diversifying our results is Maximal Marginal Relevance (MMR). MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. Fortunately, a keyword extraction algorithm called EmbedRank has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases.\n",
    "We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords/keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terrorist', 'taliban', 'president', 'stuttgart', 'war']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Yakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yake'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7cd11dc808be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0myake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yake'"
     ]
    }
   ],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(doc)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
